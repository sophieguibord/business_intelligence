---
title: 'Class 05: Scraping a single web page'
author: "Sophie Guibord"
date: "2026-02-16"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
    code_download: TRUE
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Scraping the BLS webpage

In this example, we will be scraping the [Top Growing Jobs BLS Web Page] (https://www.bls.gov/emp/tables/fastest-growing-occupations.htm)

```{r web_page_bls}
# Step 0: am I allowed to scrape this webpage
# This returned TRUE, which means I am allowed to scrpe the web page
robotstxt::paths_allowed(
  paths = "/emp/",
  domain = "https://www.bls.gov"
)

# Step 1: get the backend HTML page into R
# FOR THIS EXAMPLE, the simple rvest::read_html() failed with a "cannot open the connection" error, so we will start by declarinng ourselves

# request failed rvest::read_html ("https://www.bls.gov/emp/tables/fastest-growing-occupations.htm")

url <- "https://www.bls.gov/emp/tables/fastest-growing-occupations.htm"

resp <- httr2::request(url) |>
  httr2::req_user_agent("An ISA 401 Scraper from Miami University") |>
#  httr2::req_retry(max_tries = 3) |>
#  httr2::req_timeout(60) |>
  httr2::req_perform()

# if prev step works correctly 
# page will be a list of 2 that contains: 
# (a) head [metadata]; (b) body [things that we see]

page <- resp |> httr2::resp_body_raw() |> rvest::read_html()


# Step 2: from the page, extract the part of interest

table_1 = page |> rvest::html_elements("#BLStable_2025_7_18_12_5")

# Step 3: we make this human readable
table_1_clean = table_1 |> rvest::html_table()
table_1_clean = table_1_clean[[1]]

```











